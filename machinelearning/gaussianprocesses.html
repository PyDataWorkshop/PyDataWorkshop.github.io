<h2>Gaussian Processes</h2>
<!-- ================================================================================= -->
<b><i>Gaussian Processes for Machine Learning (GPML)<i><b> is a generic supervised learning method primarily designed to solve regression problems. It has also been extended to probabilistic classification, but in the present implementation, this is only a post-processing of the regression exercise.
<p>
The advantages of Gaussian Processes for Machine Learning are:
<ul>
<li> The prediction interpolates the observations (at least for regular correlation models).</li>
<li> The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and exceedance probabilities that might be used to refit (online fitting, adaptive fitting) the prediction in some region of interest.</li>
<li> Versatile: different linear regression models and correlation models can be specified. Common models are provided, but it is also possible to specify custom models provided they are stationary.</li>
</ul>
<br>
The disadvantages of Gaussian Processes for Machine Learning include:
<ul>
<li> It is not sparse. It uses the whole samples/features information to perform the prediction.</li>
<li> It loses efficiency in high dimensional spaces â€“ namely when the number of features exceeds a few dozens. It might indeed give poor performance and it loses computational efficiency.</li>
<li> Classification is only a post-processing, meaning that one first need to solve a regression problem by providing the complete scalar float precision output y of the experiment one attempt to model.</li>
</ul>

Thanks to the Gaussian property of the prediction, it has been given varied applications: e.g. for global optimization, probabilistic classification.

