1.3. Stochastic Gradient Descent
======================================
http://scikit-learn.org/stable/modules/sgd.html#sgd

<pre><code>
1.3. Stochastic Gradient Descent
  1.3.1. Classification
  1.3.2. Regression
  1.3.3. Stochastic Gradient Descent for sparse data
  1.3.4. Complexity
  1.3.5. Tips on Practical Use
  1.3.6. Mathematical formulation
    1.3.6.1. SGD
  1.3.7. Implementation details
</code></pre>

Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to discriminative learning of linear classifiers under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning.
SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. Given that the data is sparse, the classifiers in this module easily scale to problems with more than 10^5 training examples and more than 10^5 features.

The advantages of Stochastic Gradient Descent are:
- Efficiency.
- Ease of implementation (lots of opportunities for code tuning).
The disadvantages of Stochastic Gradient Descent include:
- SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.
- SGD is sensitive to feature scaling.


<hr>
### 1.3.3. Stochastic Gradient Descent for sparse data

Note The sparse implementation produces slightly different results than the dense implementation due to a shrunk learning rate for the intercept.

There is built-in support for sparse data given in any matrix in a format supported by scipy.sparse. For maximum efficiency, however, use the CSR matrix format as defined in scipy.sparse.csr_matrix.

Examples:
Classification of text documents using sparse features

<hr>
### 1.3.7 Implementation details

- The implementation of SGD is influenced by the Stochastic Gradient SVM of Léon Bottou. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. 
- In the case of sparse feature vectors, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. 
- Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from Shalev-Shwartz et al. 2007. 
- For multi-class classification, a “one versus all” approach is used. 
- We use the truncated gradient algorithm proposed by Tsuruoka et al. 2009 for L1 regularization (and the Elastic Net). 
- The code is written in Cython.
